if not platform: platform = '3'

import os
ld_lib_path = 'LD_LIBRARY_PATH=%s/epics/lib/linux-x86_64:%s/pcas/lib/linux-x86_64'%((os.getenv('CONDA_PREFIX'),)*2)
epics_env = 'EPICS_PVA_ADDR_LIST=172.21.156.255'+' '+ld_lib_path
hsd_epics_env = 'EPICS_PVA_ADDR_LIST=172.21.156.255'+' '+ld_lib_path

conda_rel = os.environ['CONDA_DEFAULT_ENV']

#collect_host = os.uname()[1]
collect_host = 'drp-neh-ctl001'

groups ='3' # '0 1'
hutch, user, password = ('tmo', 'tmoopr', 'pcds')
##hutch, user, password = ('tst', 'tstopr', 'pcds')
auth = ' --user {:} --password {:} '.format(user,password)
#url  = ' --url https://pswww.slac.stanford.edu/ws-auth/devlgbk/ '
#cdb  = 'https://pswww.slac.stanford.edu/ws-auth/devconfigdb/ws'
url  = ' --url https://pswww.slac.stanford.edu/ws-auth/lgbk/ '
cdb  = 'https://pswww.slac.stanford.edu/ws-auth/configdb/ws'

#
#  drp variables
#
prom_dir = '/cds/group/psdm/psdatmgr/etc/config/prom' # Prometheus
data_dir = '/ffb01/data'

task_set = 'taskset 0xffbfeffbfe '
std_opts = '-P '+hutch+' -C '+collect_host+' -M '+prom_dir+' -o '+data_dir+' -d /dev/datadev_%d'

drp_cmd0 = task_set+'drp '      +(std_opts%0)+' -k batching=no '
drp_cmd1 = task_set+'drp '      +(std_opts%1)+' -k batching=no '
pva_cmd0 = task_set+'drp_pva '  +(std_opts%0)+' -k batching=no '
pva_cmd1 = task_set+'drp_pva '  +(std_opts%1)+' -k batching=no '
bld_cmd0 = task_set+'drp_bld '  +(std_opts%0)+' -k batching=no,interface=eno1 '
bld_cmd1 = task_set+'drp_bld '  +(std_opts%1)+' -k batching=no,interface=eno1 '

ea_cfg = '/cds/group/pcds/dist/pds/tmo/misc/epicsArch_tmo.txt'
#ea_cfg = '/cds/group/pcds/dist/pds/tmo/scripts/epicsArch_test.txt'
ea_cmd0 = task_set+'epicsArch '+(std_opts%1)+' -k batching=no '+ea_cfg
ea_cmd1 = task_set+'epicsArch '+(std_opts%1)+' -k batching=no '+ea_cfg

#
#  ami variables
#
heartbeat_period = 1000 # units are ms

ami_workers_per_node = 4
#ami_worker_nodes = ["drp-neh-cmp007"]
ami_worker_nodes = ["drp-neh-cmp016"]
ami_num_workers = len(ami_worker_nodes)
#ami_manager_node = "drp-neh-cmp011"
ami_manager_node = "drp-neh-cmp016"

# procmgr FLAGS: <port number> static port number to keep executable
#                              running across multiple start/stop commands.
#                "X" open xterm
#                "s" send signal to child when stopping
#
# HOST       UNIQUEID      FLAGS  COMMAND+ARGS
# list of processes to run
#   required fields: id, cmd
#   optional fields: host, port, flags, conda, env, rtprio
#     flags:
#        'x' or 'X'  -> xterm: open small or large xterm for process console
#        's'         -> stop: sends ctrl-c to process
#        'u'         -> uniqueid: use 'id' as detector alias (supported by acq, cam, camedt, evr, and simcam)

procmgr_config = [
# {                         id:'xpmpva' ,     flags:'s',   conda:conda_rel, env:epics_env, cmd:'xpmpva DAQ:NEH:XPM:0 DAQ:NEH:XPM:1'},
 {                         id:'groupca',     flags:'s',   conda:conda_rel, env:epics_env, cmd:'groupca DAQ:NEH 0 '+groups}, # +' --prod'},
 {                         id:'procstat',    flags:'p',   conda:conda_rel,                cmd:'procstat p'+platform+'.cnf.last'},

 { host: collect_host,     id:'control',     flags:'spu', conda:conda_rel, env:epics_env, cmd:f'control -P {hutch} -B DAQ:NEH -x 0 -C BEAM {auth} {url} -d {cdb}/configDB -r /dev/null -t trigger -S 1'},
 {                         id:'control_gui', flags:'p',   conda:conda_rel,                cmd:f'control_gui -H {collect_host} --uris {cdb} --expert {auth} --loglevel WARNING'},

# { host: 'drp-neh-cmp011', id:'teb0',        flags:'spu', conda:conda_rel, cmd:task_set+'teb '+'-P '+hutch+' -C '+collect_host+' -M '+prom_dir},
# { host: 'drp-neh-cmp016', id:'teb0',        flags:'spu', conda:conda_rel, cmd:task_set+'teb '+'-P '+hutch+' -C '+collect_host+' -M '+prom_dir},

# { host: 'drp-neh-cmp001', id:'timing_0',    flags:'spu', conda:conda_rel, cmd:drp_cmd1+' -D ts -l 0x1'},
 { host: 'drp-neh-cmp002', id:'timing_0',    flags:'spu', conda:conda_rel, cmd:drp_cmd1+' -D ts -l 0x8'},
# { host: 'drp-neh-cmp014', id:'timing_0',    flags:'spu', conda:conda_rel, cmd:drp_cmd1+' -D ts -l 0x8'},

# { host: 'drp-neh-cmp001', id:'bld_0',       flags:'spu', conda:conda_rel, env:epics_env, cmd:bld_cmd1+' -l 0x2 -D gmd,xgmd'},

# { host: 'drp-neh-cmp001', id:'gmdstr0_0',   flags:'spu', conda:conda_rel, env:epics_env, cmd:pva_cmd1+' -l 0x4 ca/EM1K0:GMD:HPS:STR0:STREAM_SHORT0 -1'},   # Electron waveforms
# { host: 'drp-neh-cmp001', id:'gmdstr2_0',   flags:'spu', conda:conda_rel, env:epics_env, cmd:pva_cmd0+' -l 0x1 ca/EM1K0:GMD:HPS:STR0:STREAM_SHORT2 -1'},   # Ion waveforms
# { host: 'drp-neh-cmp002', id:'xgmdstr0_0',  flags:'spu', conda:conda_rel, env:epics_env, cmd:pva_cmd0+' -l 0x2 ca/EM2K0:XGMD:HPS:STR0:STREAM_SHORT0  -1'}, # Electron 1 waveforms
# { host: 'drp-neh-cmp014', id:'xgmdstr1_0',  flags:'spu', conda:conda_rel, env:epics_env, cmd:pva_cmd0+' -l 0x4 ca/EM2K0:XGMD:HPS:STR0:STREAM_SHORT1  -1'}, # Electron 2 waveforms
# { host: 'drp-neh-cmp015', id:'xgmdstr2_0',  flags:'spu', conda:conda_rel, env:epics_env, cmd:pva_cmd0+' -l 0x8 ca/EM2K0:XGMD:HPS:STR0:STREAM_SHORT2  -1'}, # Ion waveforms

# #{ host: 'drp-neh-cmp002', id:'epics_0',     flags:'spu', conda:conda_rel, env:epics_env, cmd:ea_cmd1+' -l 0x1'},
# { host: 'drp-neh-cmp001', id:'epics_0',     flags:'spu', conda:conda_rel, env:epics_env, cmd:ea_cmd1+' -l 0x8'},
#
# { host: 'drp-neh-cmp002', id:'im1k0_0',     flags:'spu',  conda:conda_rel, env:epics_env, cmd:pva_cmd1+' -l 0x2 IM1K0:XTES:CAM:IMAGE1:Pva:Image -k batching=no,firstdim=1024 -1'},
#
## { host: 'drp-neh-cmp002', id:'im2k0_0',     flags:'spu',  conda:conda_rel, env:epics_env, cmd:pva_cmd1+' -l 0x4 IM2K0:XTES:CAM:IMAGE1:Pva:Image -k batching=no,firstdim=1024 -0'},
#
## { host: 'drp-neh-cmp002', id:'im1k4_0',     flags:'spu',  conda:conda_rel, env:epics_env, cmd:pva_cmd1+' -l 0x8 IM1K4:XTES:CAM:IMAGE1:Pva:Image -k batching=no,firstdim=1024 -0'},
#
# #{ host: 'drp-neh-cmp001', id:'im2k4_0',     flags:'spu',  conda:conda_rel, env:epics_env, cmd:pva_cmd1+' -l 0x8 IM2K4:PPM:CAM:DATA1:Pva:Image -k batching=no,firstdim=2048,pebbleBufSize=8388672 -1'},
# { host: 'drp-neh-cmp002', id:'im2k4_0',     flags:'spu',  conda:conda_rel, env:epics_env, cmd:pva_cmd1+' -l 0x1 IM2K4:PPM:CAM:DATA1:Pva:Image -k batching=no,firstdim=2048,pebbleBufSize=8388672 -1'},

# { host: 'drp-neh-cmp011', id:'tmoopal_0',   flags:'spu',  conda:conda_rel, env:epics_env, cmd:drp_cmd0+' -D opal -l 0x1'},
]

#hsd_epics = 'DAQ:TMO:HSD:1'
#node=0
#segm=1
#hsds = [
##{node:'drp-neh-cmp017',segm:[('1A:B','hsd_1'),('1A:A','hsd_0')]},
#{node:'drp-neh-cmp020',segm:[('1B:B','hsd_3'),('1B:A','hsd_2')]},
##{node:'drp-neh-cmp022',segm:[('5E:B','hsd_5'),('5E:A','hsd_4')]},
#]
#
#pvlist = []
#for e in hsds:
##    procmgr_config.append({host:e[node], id:e[segm][0][1], flags:'spu', conda:conda_rel, env:epics_env, cmd:drp_cmd0+' -D hsd -k hsd_epics_prefix={:}_{:}'.format(hsd_epics,e[segm][0][0])})
#    procmgr_config.append({host:e[node], id:e[segm][1][1], flags:'spu', conda:conda_rel, env:epics_env, cmd:drp_cmd1+' -D hsd -k hsd_epics_prefix={:}_{:}'.format(hsd_epics,e[segm][1][0])})
#    pvlist.append('{:}_{:}'.format(hsd_epics,e[segm][1][0]))
#
#if pvlist != []:
#    procmgr_config.append({id:'hsdpva', flags:'', env:epics_env, cmd:'hsdpva '+' '.join(pvlist)})
#
procmgr_ami = [
#
# ami
#
 { 'host': ami_manager_node, 'id':'ami-global',  'flags':'s', 'conda':conda_rel, 'cmd':f'ami-global -N 0 -n {ami_num_workers}'},
 { 'host': ami_manager_node, 'id':'ami-manager', 'flags':'s', 'conda':conda_rel, 'cmd':f'ami-manager -n {ami_num_workers*ami_workers_per_node} -N {ami_num_workers}'},
 {                           'id':'ami-client',  'flags':'s', 'conda':conda_rel, 'cmd':f'ami-client -H {ami_manager_node}'},
]
#
# ami workers
#

for N, worker_node in enumerate(ami_worker_nodes):
    procmgr_ami.append({ host: worker_node, id:f'meb{N}', flags:'spu', conda:conda_rel,
                         cmd:f'{task_set} monReqServer -P {hutch} -C {collect_host} -M {prom_dir} -d -q {ami_workers_per_node}'})
    for n in range(ami_workers_per_node):
      procmgr_ami.append({ 'host': worker_node, 'id': f'ami-worker_{N}_{n}', 'flags':'s', 'conda':conda_rel,
                           'cmd': f'ami-worker -N {N*ami_num_workers+n} -n {ami_num_workers} -b {heartbeat_period} -H {ami_manager_node} psana:////reg/g/pcds/dist/pds/tmo/misc/worker.json'})

    procmgr_ami.append({ 'host': worker_node, 'id': f'ami-node_{N}', 'flags':'s', 'conda':conda_rel,
    	                 'cmd': f'ami-node -N {N} -n {ami_workers_per_node} -H {ami_manager_node} --log-level debug'})

#procmgr_config.extend(procmgr_ami)
